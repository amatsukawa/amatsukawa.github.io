<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Variational Dequantization - Akihiro Matsukawa</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Variational Dequantization" />
<meta property="og:description" content="Training a dequantizer to fit a continuous distribution to discrete data (Flow&#43;&#43; paper)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://amatsukawa.github.io/posts/variational-dequantizer/" />
<meta property="article:published_time" content="2019-02-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-02-28T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Variational Dequantization"/>
<meta name="twitter:description" content="Training a dequantizer to fit a continuous distribution to discrete data (Flow&#43;&#43; paper)."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://amatsukawa.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://amatsukawa.github.io/css/main.css" />

	<script src="https://amatsukawa.github.io/js/feather.min.js"></script>
	
	<script src="https://amatsukawa.github.io/js/main.js"></script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="https://amatsukawa.github.io/">Akihiro Matsukawa</a></h1>
	<div class="site-description"><nav class="nav social">
			<ul class="flat"><a href="https://github.com/amatsukawa" title="Github"><i data-feather="github"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Blog</a>
			</li>
			
			<li>
				<a href="/notes/">Notes</a>
			</li>
			
			<li>
				<a href="/pub/">Publications</a>
			</li>
			
			<li>
				<a href="/about/">About</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Variational Dequantization</h1>
			<div class="meta">
                          
                            Feb 28, 2019
                          
                          
                        </div>
		</div>

		<div class="markdown">
			<p>In this post I&rsquo;ll discuss methods for dequantizing discrete values for continuous distributions.
We&rsquo;ll start with why dequantization is needed, then move on to a simple method for the problem,
and end with a more flexible and general method recently proposed in the Flow++ paper [1].</p>
<p>Most measurements in the real world are continuous, and therefore we may want to use a continuous
distribution to model them. However, for the sake of storage, most measurement values are clipped to
a pre-defined discrete set of values (aka. <em>quantized</em>). For instance, we typicall quantize images
to an 8-bit integer in the range $[0, 255]$.</p>
<p>If we naively fit a continuous distribution to these quantized values, the model can learn to achieve
high lihelihood by placing large spikes at these quantized values, while making the likelihood low
everywhere else. This is an unnatural distribution that is unlikely to appear in the real world,
and we would like to discourage our model from overfitting to the quantization. For the sake of
simplicity, let&rsquo;s assume our data is images, and has been quantized to the $[0, 255]$ in
increments of 1.</p>
<p align="center"> 
<img src="/assets/vardeq/nodeq.png">
</p>
<p>What might we do to prevent these spikes at the quantization centers? You might intuitively think to
add some noise to smooth the values. $u \sim Uniform(0, 1)$ seems like a decent choice, since we just
&ldquo;fill&rdquo; those quantization buckets equally. This is in fact the
most common approach to dequantize discrete values for a continuous distribution [2].
However, this approach introduces flat step-wise regions into the data distribution, and while it&rsquo;s better
than having spikes, it is also unnatural and difficult to fit most parametric distributions.</p>
<p align="center"> 
<img src="/assets/vardeq/uniform_deq.png">
</p>
<p>How do we improve this? We still want to add some smoothing noise $u$, but we don&rsquo;t want it to
be uniform noise around the quantization centers. So let&rsquo;s make it a distribution that is dependent on
what the value of $x$ is, and let&rsquo;s learn that distribution, $u \sim r(u|x)$.</p>
<p>In order to learn $r(u|x)$, we need a loss function. It turns out the &ldquo;hack&rdquo; of adding quantization noise,
learned or not, upper bounds the continuous data likelihood by the discrete
data likelihood.</p>
<p>We want to learn a continuous distribution $q(x)$ for the data. Since the data we have are actually
discrete and quantized, let&rsquo;s say $q$ integrated over the quantization bin gives you the discrete distribution:</p>
<p>$$
Q(x) = \int_{[0, 1)} q(x+u) du
$$</p>
<p>Let&rsquo;s define $P(x)$ as the discrete, quantized data distribution that we are trying to fit.
The discrete maximum likelihood estiamtor is $ \mathop{\mathbb{E}}_{x \sim P(x)}[Q(X)] $</p>
<p>$$
\begin{aligned}
\mathop{\mathbb{E}}<em>{x \sim P(x)}[Q(X)]
&amp;= \mathop{\mathbb{E}}</em>{x \sim P(x)}\left[\log \int_{[0, 1)} q(x+u) du\right] \\<br>
&amp;= \mathop{\mathbb{E}}_{x \sim P(x)}\left[\log \int_{[0, 1)} r(u|x) \frac{q(x+u)}{r(u|x)} du\right] \\<br>
&amp;= \mathop{\mathbb{E}}_{x \sim P(x)}\left[\log \mathop{\mathbb{E}}_{u \sim r(u|x)} \left[\frac{q(x+u)}{r(u|x)} \right]\right] \\<br>
&amp;\ge \mathop{\mathbb{E}}_{x \sim P(x)}\mathop{\mathbb{E}}_{u \sim r(u|x)} \left[\log \frac{q(x+u)}{r(u|x)} \right] \\<br>
&amp;= \mathop{\mathbb{E}}_{x \sim P(x)}\mathop{\mathbb{E}}_{u \sim r(u|x)} \left[\log q(x+u) - \log r(u|x) \right]
\end{aligned}
$$</p>
<p>What we&rsquo;ve done here is introduced a variational distribution to model the noise. If we use a
flow for $r(u|x)$, we can train the model and dequantizer together using the path-wise gradient estiamtor,
or the &ldquo;reparameterization trick&rdquo; as it&rsquo;s known in VAEs.
Note that the using uniform noise to dequantize is simply choosing the uniform distribution to be $r$.</p>
<p align="center"> 
<img src="/assets/vardeq/flowpp_deq.png">
</p>
<p>By jointly training the noise distribution model and the data distribution model, we can find a dequantization method that is both
a valid upper bound for the likelihood and is more natural for the data distribution to fit to.</p>
<h3 id="references">References</h3>
<p><a href="https://arxiv.org/abs/1902.00275">[1] Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design</a></p>
<p><a href="https://arxiv.org/abs/1511.01844">[2] A note on the evaluation of generative models</a></p>

		</div>

		<div class="post-tags">
			
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>



<script>feather.replace()</script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</body>
</html>
